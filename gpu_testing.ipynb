{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/ianrist/dev/swli_analysis/.venv/lib/python3.11/site-packages/torchaudio/lib/libtorchaudio.so, 0x0006): Symbol not found: __ZN3c1015SmallVectorBaseIjE8grow_podEPvmm\n  Referenced from: <29A427D4-A7A1-3786-B07A-754A9DF07FAD> /Users/ianrist/dev/swli_analysis/.venv/lib/python3.11/site-packages/torchaudio/lib/libtorchaudio.so\n  Expected in:     <5445D2E4-6D7A-39F2-9003-F3A3F854555A> /Users/ianrist/dev/swli_analysis/.venv/lib/python3.11/site-packages/torch/lib/libc10.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hilbert, butter, filtfilt\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n",
      "File \u001b[0;32m~/dev/swli_analysis/.venv/lib/python3.11/site-packages/torchaudio/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     _extension,\n\u001b[1;32m      3\u001b[0m     compliance,\n\u001b[1;32m      4\u001b[0m     datasets,\n\u001b[1;32m      5\u001b[0m     functional,\n\u001b[1;32m      6\u001b[0m     io,\n\u001b[1;32m      7\u001b[0m     kaldi_io,\n\u001b[1;32m      8\u001b[0m     models,\n\u001b[1;32m      9\u001b[0m     pipelines,\n\u001b[1;32m     10\u001b[0m     sox_effects,\n\u001b[1;32m     11\u001b[0m     transforms,\n\u001b[1;32m     12\u001b[0m     utils,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_audio_backend, list_audio_backends, set_audio_backend\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/swli_analysis/.venv/lib/python3.11/site-packages/torchaudio/_extension/__init__.py:43\u001b[0m\n\u001b[1;32m     41\u001b[0m _IS_KALDI_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_TORCHAUDIO_EXT_AVAILABLE:\n\u001b[0;32m---> 43\u001b[0m     \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchaudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torchaudio\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     _check_cuda_version()\n",
      "File \u001b[0;32m~/dev/swli_analysis/.venv/lib/python3.11/site-packages/torchaudio/_extension/utils.py:61\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m torch\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/swli_analysis/.venv/lib/python3.11/site-packages/torch/_ops.py:1357\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1352\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m-> 1357\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ctypes/__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/ianrist/dev/swli_analysis/.venv/lib/python3.11/site-packages/torchaudio/lib/libtorchaudio.so, 0x0006): Symbol not found: __ZN3c1015SmallVectorBaseIjE8grow_podEPvmm\n  Referenced from: <29A427D4-A7A1-3786-B07A-754A9DF07FAD> /Users/ianrist/dev/swli_analysis/.venv/lib/python3.11/site-packages/torchaudio/lib/libtorchaudio.so\n  Expected in:     <5445D2E4-6D7A-39F2-9003-F3A3F854555A> /Users/ianrist/dev/swli_analysis/.venv/lib/python3.11/site-packages/torch/lib/libc10.dylib"
     ]
    }
   ],
   "source": [
    "import fft_interp\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.signal import hilbert, butter, filtfilt, sosfiltfilt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = fft_interp.get_frames(\"videos/rawglasssmall.avi\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = frames[:, :50, :50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hilbert_transform_1d_torch(data_torch, axis: int = -1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the 1D Hilbert transform of a 3D real array along the specified axis\n",
    "    using PyTorch's FFT operations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_np : np.ndarray\n",
    "        A 3D real-valued NumPy array (e.g. shape (X, Y, Z)).\n",
    "    axis : int\n",
    "        The axis along which to compute the 1D Hilbert transform.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hilbert : np.ndarray\n",
    "        A PyTorch tensor containing the Hilbert transform of data_np\n",
    "        along the specified axis. The shape matches data_np, but the dtype\n",
    "        is float (matching the imaginary result of the inverse FFT).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the NumPy array to a PyTorch tensor (float or double)\n",
    "    # We'll assume float32 here; adjust as needed\n",
    "    # data_torch = torch.from_numpy(data_np).to(torch.float32)\n",
    "\n",
    "    # FFT along the chosen axis\n",
    "    data_fft = torch.fft.fft(data_torch, dim=axis)\n",
    "\n",
    "    # Prepare the frequency-domain multiplier for the Hilbert transform\n",
    "    n = data_torch.size(axis)\n",
    "\n",
    "    # Create an empty complex filter (shape = n), initially zeros\n",
    "    hilb_filter = torch.zeros(n, dtype=torch.complex64, device=data_fft.device)\n",
    "\n",
    "    # Handle even/odd length along 'axis'\n",
    "    #   - DC component (k=0) and (if even length) Nyquist freq (k=n/2) remain 0\n",
    "    #   - For 1 <= k < n/2: multiply by -j\n",
    "    #   - For n/2 < k < n: multiply by +j\n",
    "    if n % 2 == 0:\n",
    "        # Even number of points\n",
    "        #  - Positive freqs are indices [1 ... n/2 - 1]\n",
    "        #  - Nyquist freq is index n/2\n",
    "        hilb_filter[1 : (n // 2)] = -1j\n",
    "        hilb_filter[(n // 2 + 1) : ] = 1j\n",
    "    else:\n",
    "        # Odd number of points\n",
    "        #  - Positive freqs are indices [1 ... (n-1)//2]\n",
    "        #  - Negative freqs are indices [(n+1)//2 ... n-1]\n",
    "        half_n = (n + 1) // 2\n",
    "        hilb_filter[1 : half_n] = -1j\n",
    "        hilb_filter[half_n : ] = 1j\n",
    "\n",
    "    # Reshape the filter so it can broadcast along 'axis' in a 3D tensor\n",
    "    # Build a shape of [1,1,1] and replace the dimension at 'axis' with n\n",
    "    shape = [1, 1, 1]\n",
    "    shape[axis] = n\n",
    "    hilb_filter = hilb_filter.reshape(shape)\n",
    "\n",
    "    # Apply the Hilbert filter in the frequency domain\n",
    "    data_fft_filtered = data_fft * hilb_filter\n",
    "\n",
    "    # Inverse FFT to get the Hilbert transform in time/space domain\n",
    "    # The result is, in general, a complex tensor whose imaginary part\n",
    "    # corresponds to the Hilbert transform of the original data.\n",
    "    data_ifft = torch.fft.ifft(data_fft_filtered, dim=axis)\n",
    "\n",
    "    return data_ifft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_frames = torch.tensor(frames, dtype=torch.float32)\n",
    "gpu_frames = gpu_frames.to(mps_device)\n",
    "hilbert_ed = hilbert_transform_1d_torch(gpu_frames, axis=0)\n",
    "hilbert_ed = hilbert_ed.cpu().numpy()\n",
    "# plot for one pixel to debug\n",
    "\n",
    "hilbert_ed = np.abs(hilbert_ed)\n",
    "\n",
    "sos = butter(2, 0.01, btype='lowpass', output='sos')\n",
    "\n",
    "filtered_envelope = sosfiltfilt(sos, hilbert_ed, axis=0)\n",
    "height_map = np.argmax(filtered_envelope, axis=0)\n",
    "\n",
    "SCAN_SPEED = 0.25  # Microns per second\n",
    "FPS = 30\n",
    "\n",
    "microns_per_frame = SCAN_SPEED * 1 / FPS\n",
    "\n",
    "height_map = height_map *microns_per_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_interp.plot_height_map(height_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(y=hilbert_ed[:, 0, 0])\n",
    "fig.add_scatter(y=frames[:, 0, 0])\n",
    "fig.add_scatter(y=filtered_envelope[:, 0, 0])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
